{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "from rl_reinforce import REINFORCEothello\n",
    "from othello_net import *\n",
    "from othello_rules import *\n",
    "from feature_extractor import *\n",
    "import os, random, sys, gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from example_states import *\n",
    "from training_utils import *\n",
    "np.set_printoptions(precision=2)\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting self play\n",
      "Step: 0\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 5\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 10\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 15\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 20\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 25\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 30\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 35\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 40\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 45\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 50\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "Step: 55\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 60\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 65\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 70\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "Step: 75\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "Step: 80\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 85\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 90\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "Step: 95\n",
      "graph 1 wins\n",
      "graph 2 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "graph 1 wins\n",
      "16. 20:43:46  a_player_0.ckpt wr: 0.88, b_player_0.ckpt wr: 0.10\n"
     ]
    }
   ],
   "source": [
    "def sample_action(prediction, player):\n",
    "    prediction = np.transpose(prediction[0])\n",
    "    prediction = np.transpose(prediction[1])\n",
    "    legal_moves = find_legal_moves(board, player)\n",
    "    prediciton = prediction / np.sum(prediction)\n",
    "    cleaned_predictions = zero_illegal_moves(prediction, legal_moves)\n",
    "    if player == graph_1 and debug:\n",
    "        print(\"cleaned predictions\")\n",
    "        print(cleaned_predictions)\n",
    "    p = cleaned_predictions.flatten()\n",
    "    p = p / np.sum(p)\n",
    "    sample_index = np.flatnonzero( np.random.multinomial(1,p,1) )[0]\n",
    "    sampled_move = moves[sample_index]\n",
    "    if prev_move == sampled_move:\n",
    "        print(\"action probabilities\")\n",
    "        print(p)\n",
    "        print(prediction)\n",
    "        print(cleaned_predictions)\n",
    "    return sampled_move\n",
    "\n",
    "moves = ['0'] * 64\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        moves[i*8 + j] = str((i+1) * 10 + (j+1))\n",
    "\n",
    "ops.reset_default_graph()\n",
    "\n",
    "graph_1, img_data_1, train_step_1, optimizer_1, ground_truths_1, loss_1, pred_up_1, keep_prob_1, learn_rate_1, score_out_1 = create_othello_net()\n",
    "sess_1 = tf.Session(graph=graph_1)\n",
    "saver_1 = tf.train.Saver()\n",
    "init_op_1 = tf.initialize_all_variables()\n",
    "sess_1.run(init_op_1)\n",
    "model_1 = \"models/rl-p-a/a_player_0.ckpt\"\n",
    "if os.path.isfile(model_1):\n",
    "    saver_1.restore(sess_1, model_1)\n",
    "\n",
    "graph_2, img_data_2, train_step_2, optimizer_2, ground_truths_2, loss_2, pred_up_2, keep_prob_2, learn_rate_2, score_out_2 = create_othello_net()\n",
    "sess_2 = tf.Session(graph=graph_2)\n",
    "saver_2 = tf.train.Saver()\n",
    "init_op_2 = tf.initialize_all_variables()\n",
    "sess_2.run(init_op_2)\n",
    "model_2 = 'models/rl-p-b/b_player_0.ckpt'\n",
    "if os.path.isfile(model_2):\n",
    "    saver_2.restore(sess_2, model_2)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "graph3, img_data3, train_step3, optimizer3, ground_truths3, loss3, pred_up3, keep_prob3, learn_rate3, score_out3 = create_value_net()\n",
    "saver3 = tf.train.Saver()\n",
    "sess3 = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "init_op3 = tf.initialize_all_variables()\n",
    "sess3.run(init_op3)\n",
    "current_model3 = \"models/sl-v/players_g.ckpt.b\"\n",
    "if os.path.isfile(current_model3):\n",
    "    saver3.restore(sess3, current_model3)\n",
    "    \n",
    "        \n",
    "batches = 1\n",
    "\n",
    "debug = False\n",
    "#mini = True\n",
    "use_value_network = True\n",
    "random_player = False\n",
    "\n",
    "N = 1 if debug else 100\n",
    "print(\"starting self play\")\n",
    "\n",
    "\n",
    "for batch in range(batches):\n",
    "    graph_1 = -1\n",
    "    graph_2 = 1\n",
    "    graph_1_wins = 0\n",
    "    graph_2_wins = 0\n",
    "    for n in range(N):\n",
    "        if n%5 is 0:\n",
    "            print(\"Step: \" + str(n))\n",
    "        board = initialize_game()\n",
    "        player = -1\n",
    "        prev_move = '00'\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            cnt+=1\n",
    "            legal_moves = find_legal_moves(board, player)\n",
    "            if len(legal_moves) == 0:\n",
    "                winner = get_winner(board, 1, -1)\n",
    "                if winner is graph_1:\n",
    "                    print(\"graph 1 wins\")\n",
    "                    graph_1_wins += 1\n",
    "                if winner is graph_2:\n",
    "                    print(\"graph 2 wins\")\n",
    "                    graph_2_wins += 1\n",
    "                break\n",
    "            features = board_to_input(board, player)\n",
    "            if player is graph_1:\n",
    "                if use_value_network:\n",
    "                    #sampled_move  = minimax_search(board, 1, player, player)\n",
    "                    sampled_move  = alphabeta_search(board, 1, player, player)\n",
    "                else:\n",
    "                    prediction = sess_1.run(pred_up_1, feed_dict={img_data_1:[features], keep_prob_1:1.0})\n",
    "                    sampled_move = sample_action(prediction, player)\n",
    "            else:\n",
    "                prediction = sess_2.run(pred_up_2, feed_dict={img_data_2:[features], keep_prob_2:1.0})\n",
    "                sampled_move = sample_action(prediction, player)\n",
    "            \n",
    "            if prev_move == sampled_move:\n",
    "                print(\"illegal action sampled\")\n",
    "                break\n",
    "                \n",
    "            prev_move = sampled_move\n",
    "            board = make_move(board, sampled_move, player, debug=True)\n",
    "            player = -1 if player is 1 else 1\n",
    "            legal_moves = find_legal_moves(board, player)\n",
    "            if len(legal_moves) == 0:\n",
    "                player = -1 if player is 1 else 1\n",
    "\n",
    "        graph_1 = graph_1 * (-1)\n",
    "        graph_2 = graph_2 * (-1)\n",
    "        if n+1 is N:\n",
    "            print('%s  %s wr: %.2f, %s wr: %.2f' % (datetime.now().strftime(\"%d. %H:%M:%S\"), model_1[14:], graph_1_wins/float(N), model_2[14:], graph_2_wins/float(N)))\n",
    "            graph_1_wins = 0\n",
    "            graph_2_wins = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MINIMAX: 15. 17:44:39  a_player_0.ckpt wr: 0.93, b_player_0.ckpt wr: 0.05\n",
    "# ONLYMAX: 15. 18:04:49  a_player_0.ckpt wr: 0.43, b_player_0.ckpt wr: 0.57\n",
    "# RANDOM : 15. 18:14:16  a_player_0.ckpt wr: 0.27, b_player_0.ckpt wr: 0.71\n",
    "# ...\n",
    "# minimax with depth   = 0 : 6: 17:51:29  a_player_0.ckpt wr: 0.49, b_player_0.ckpt wr: 0.50\n",
    "# alphabeta with depth = 1 : 16. 20:43:46  a_player_0.ckpt wr: 0.88, b_player_0.ckpt wr: 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_value_net():\n",
    "    with tf.variable_scope(\"value_network\"):\n",
    "        img_data = tf.placeholder(tf.float32, shape=[None, 8, 8, 28], name=\"img_data\")\n",
    "        keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        #convolutional layers \n",
    "        i = 28; o = 32;\n",
    "        conv1_weights = conv_weights(i, o)\n",
    "        conv1_biases = conv_biases(o)\n",
    "        conv1_out = conv_nonparams2(img_data, conv1_weights, conv1_biases, keep_prob)\n",
    "        #pool1 = max_pool_2x2(conv1_out)\n",
    "        i = o; o = 32;\n",
    "        conv2_weights = conv_weights(i, o)\n",
    "        conv2_biases = conv_biases(o)\n",
    "        conv2_out = conv_nonparams2(conv1_out, conv2_weights, conv2_biases, keep_prob)\n",
    "        #pool2 = max_pool_2x2(conv2_out)\n",
    "        i = o; o = 32;\n",
    "        conv3_weights = conv_weights2(i, o)\n",
    "        conv3_biases = conv_biases(o)\n",
    "        conv3_out = conv_nonparams2(conv2_out, conv3_weights, conv3_biases, keep_prob)\n",
    "        \n",
    "        i = o; o = 32;\n",
    "        conv4_weights = conv_weights2(i, o)\n",
    "        conv4_biases = conv_biases(o)\n",
    "        conv4_out = conv_nonparams2(conv3_out, conv4_weights, conv4_biases, keep_prob)\n",
    "        \n",
    "        k = 1; i = o; o = 2;\n",
    "        s_weights = weight_variable([k, k, i, o])\n",
    "        s_biases = bias_variable([o])\n",
    "        s_out = conv_nonparams(conv4_out, s_weights, s_biases, keep_prob)\n",
    "        \n",
    "        k = 8; i = o; o = 2;\n",
    "        score_weights = weight_variable([k, k, i, o])\n",
    "        score_biases = bias_variable([o])\n",
    "        score_out = conv_nonparams3(s_out, score_weights, score_biases, keep_prob)\n",
    "        score_out_flat = tf.reshape(score_out, [-1, o])\n",
    "        #final layer\n",
    "        #Don't actually use this layer here\n",
    "        predictions = softmax(score_out, o)\n",
    "\n",
    "        #training block:\n",
    "        label = tf.placeholder(tf.float32, shape=[None, 2], name=\"ground_truths\")\n",
    "        learn_rate = tf.placeholder(tf.float32, name=\"eta\")\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(score_out_flat, label))\n",
    "        policy_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        reg_loss = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_network_variables])\n",
    "        optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        return sess.graph, img_data, train_step, optimizer, label, loss, predictions, keep_prob, learn_rate, score_out\n",
    "    \n",
    "def list_softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    sf = np.exp(x)\n",
    "    sf = sf/np.sum(sf, axis=0)\n",
    "    return sf\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x)\n",
    "\n",
    "def conv_nonparams2(tensor_input, conv_weights, conv_biases, keep_prob):\n",
    "    conv_in = conv(tensor_input, conv_weights)\n",
    "    conv_relu = lrelu(conv_in + conv_biases)\n",
    "    conv_drop = tf.nn.dropout(conv_relu, keep_prob)\n",
    "    return conv_drop\n",
    "\n",
    "def conv_weights2(i, o):\n",
    "    k = 2\n",
    "    shape = [k, k, i, o]\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv_nonparams3(tensor_input, conv_weights, conv_biases, keep_prob):\n",
    "    conv_in = conv(tensor_input, conv_weights, \"VALID\")\n",
    "    conv_relu = lrelu(conv_in + conv_biases)\n",
    "    conv_drop = tf.nn.dropout(conv_relu, keep_prob)\n",
    "    return conv_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def minimax_search(state, depth, maximizing_player, player_to_move):\n",
    "    legal_moves = find_legal_moves(state, player_to_move)\n",
    "    best_val = 0\n",
    "    best_move = random.choice(legal_moves)\n",
    "    for move in legal_moves:\n",
    "        minimax_board = make_move(state, move, maximizing_player, debug=True)\n",
    "        minimax_value = minimax(minimax_board, depth, maximizing_player, player_to_move * (-1))\n",
    "        if minimax_value > best_val:\n",
    "            best_val  = minimax_value\n",
    "            best_move = move    \n",
    "    return best_move\n",
    "\n",
    "def alphabeta_search(state, depth, maximizing_player, player_to_move):\n",
    "    legal_moves = find_legal_moves(state, player_to_move)\n",
    "    best_val = 0\n",
    "    best_move = random.choice(legal_moves)\n",
    "    for move in legal_moves:\n",
    "        ab_board = make_move(state, move, maximizing_player, debug=True)\n",
    "        ab_value = alphabeta(ab_board, depth, maximizing_player, player_to_move * (-1), 0, 1)\n",
    "        if ab_value > best_val:\n",
    "            best_val  = ab_value\n",
    "            best_move = move    \n",
    "    return best_move\n",
    "\n",
    "def alphabeta(state, depth, maximizing_player, player_to_move, alpha, beta):\n",
    "    possible_actions = find_legal_moves(state, player_to_move)\n",
    "    if depth == 0 or len(legal_moves) == 0:\n",
    "        mm_features = board_to_input(state, player_to_move)\n",
    "        values = sess3.run(score_out3, feed_dict={img_data3:[mm_features], keep_prob3:1.0})\n",
    "        values = values[0][0][0]\n",
    "        soft_values = list_softmax(values)\n",
    "        if maximizing_player == 1:\n",
    "            return soft_values[0]\n",
    "        else:\n",
    "            return soft_values[1]\n",
    "    elif maximizing_player == player_to_move:\n",
    "        best_value = 0\n",
    "        for action in possible_actions:\n",
    "            possible_state = make_move(state, action, player_to_move, debug=True)\n",
    "            v = alphabeta(possible_state, depth-1, maximizing_player, player_to_move*(-1.0), alpha, beta)\n",
    "            if v > best_value:\n",
    "                best_value = v\n",
    "            if v > alpha:\n",
    "                alpha = v\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return best_value\n",
    "    else:\n",
    "        best_value = 1\n",
    "        for action in possible_actions:\n",
    "            possible_state = make_move(state, action, player_to_move, debug=True)\n",
    "            v = alphabeta(possible_state, depth-1, maximizing_player, player_to_move*(-1.0), alpha, beta)\n",
    "            if v < best_value:\n",
    "                best_value = v\n",
    "            if v < beta:\n",
    "                beta = v\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return best_value\n",
    "                \n",
    "def minimax(state, depth, maximizing_player, player_to_move):\n",
    "    possible_actions = find_legal_moves(state, player_to_move)\n",
    "    if depth == 0 or len(legal_moves) == 0:\n",
    "        mm_features = board_to_input(state, player_to_move)\n",
    "        values = sess3.run(score_out3, feed_dict={img_data3:[mm_features], keep_prob3:1.0})\n",
    "        values = values[0][0][0]\n",
    "        soft_values = list_softmax(values)\n",
    "        if maximizing_player == 1:\n",
    "            return soft_values[0]\n",
    "        else:\n",
    "            return soft_values[1]\n",
    "    elif maximizing_player == player_to_move:\n",
    "        best_value = 0\n",
    "        for action in possible_actions:\n",
    "            possible_state = make_move(state, action, player_to_move, debug=True)\n",
    "            v = minimax(possible_state, depth-1, maximizing_player, player_to_move*(-1.0))\n",
    "            if v > best_value:\n",
    "                best_value = v\n",
    "        return best_value\n",
    "    else:\n",
    "        best_value = 1\n",
    "        for action in possible_actions:\n",
    "            possible_state = make_move(state, action, player_to_move, debug=True)\n",
    "            v = minimax(possible_state, depth-1, maximizing_player, player_to_move*(-1.0))\n",
    "            if v < best_value:\n",
    "                best_value = v\n",
    "        return best_value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
