{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-08 13:17:09,946] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim: 4\n",
      "num_actions: 2\n",
      "Episode 0\n",
      "Finished after 20 timesteps\n",
      "Reward for this episode: 20.0\n",
      "Average reward for last 100 episodes: 20.0\n",
      "Episode 1\n",
      "Finished after 45 timesteps\n",
      "Reward for this episode: 45.0\n",
      "Average reward for last 100 episodes: 32.5\n",
      "Episode 2\n",
      "Finished after 26 timesteps\n",
      "Reward for this episode: 26.0\n",
      "Average reward for last 100 episodes: 30.3333333333\n",
      "Episode 3\n",
      "Finished after 30 timesteps\n",
      "Reward for this episode: 30.0\n",
      "Average reward for last 100 episodes: 30.25\n",
      "Episode 4\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode: 11.0\n",
      "Average reward for last 100 episodes: 26.4\n",
      "Episode 5\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode: 15.0\n",
      "Average reward for last 100 episodes: 24.5\n",
      "Episode 6\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode: 15.0\n",
      "Average reward for last 100 episodes: 23.1428571429\n",
      "Episode 7\n",
      "Finished after 34 timesteps\n",
      "Reward for this episode: 34.0\n",
      "Average reward for last 100 episodes: 24.5\n",
      "Episode 8\n",
      "Finished after 21 timesteps\n",
      "Reward for this episode: 21.0\n",
      "Average reward for last 100 episodes: 24.1111111111\n",
      "Episode 9\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode: 13.0\n",
      "Average reward for last 100 episodes: 23.0\n",
      "Episode 10\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode: 13.0\n",
      "Average reward for last 100 episodes: 22.0909090909\n",
      "Episode 11\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode: 15.0\n",
      "Average reward for last 100 episodes: 21.5\n",
      "Episode 12\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode: 14.0\n",
      "Average reward for last 100 episodes: 20.9230769231\n",
      "Episode 13\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode: 17.0\n",
      "Average reward for last 100 episodes: 20.6428571429\n",
      "Episode 14\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode: 15.0\n",
      "Average reward for last 100 episodes: 20.2666666667\n",
      "Episode 15\n",
      "Finished after 19 timesteps\n",
      "Reward for this episode: 19.0\n",
      "Average reward for last 100 episodes: 20.1875\n",
      "Episode 16\n",
      "Finished after 30 timesteps\n",
      "Reward for this episode: 30.0\n",
      "Average reward for last 100 episodes: 20.7647058824\n",
      "Episode 17\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode: 13.0\n",
      "Average reward for last 100 episodes: 20.3333333333\n",
      "Episode 18\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode: 12.0\n",
      "Average reward for last 100 episodes: 19.8947368421\n",
      "Episode 19\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode: 11.0\n",
      "Average reward for last 100 episodes: 19.45\n",
      "Episode 20\n",
      "Finished after 25 timesteps\n",
      "Reward for this episode: 25.0\n",
      "Average reward for last 100 episodes: 19.7142857143\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b0b1e7eda6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg_reinforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampleAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "from pg_reinforce import PolicyGradientREINFORCE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "ops.reset_default_graph()\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "sess = tf.Session()\n",
    "#saver = tf.train.Saver()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\n",
    "writer = tf.train.SummaryWriter(\"/tmp/{}-experiment-1\".format(env_name))\n",
    "\n",
    "state_dim   = env.observation_space.shape[0]\n",
    "print(\"state dim: \" + str(state_dim))\n",
    "num_actions = env.action_space.n\n",
    "print(\"num_actions: \" + str(num_actions))\n",
    "def policy_network(states):\n",
    "  # define policy neural network\n",
    "  W1 = tf.get_variable(\"W1\", [state_dim, 20],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b1 = tf.get_variable(\"b1\", [20],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n",
    "  W2 = tf.get_variable(\"W2\", [20, num_actions],\n",
    "                       initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "  b2 = tf.get_variable(\"b2\", [num_actions],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  p = tf.matmul(h1, W2) + b2\n",
    "  return p\n",
    "current_model = \"cartpole.ckpt\"\n",
    "pg_reinforce = PolicyGradientREINFORCE(sess,\n",
    "                                       optimizer,\n",
    "                                       policy_network,\n",
    "                                       current_model,\n",
    "                                       state_dim,\n",
    "                                       num_actions,\n",
    "                                       summary_writer=writer)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "MAX_STEPS    = 200\n",
    "#sys.exit(\"Error message\")\n",
    "episode_history = deque(maxlen=100)\n",
    "for i_episode in xrange(MAX_EPISODES):\n",
    "\n",
    "  # initialize\n",
    "  state = env.reset()\n",
    "  total_rewards = 0\n",
    "\n",
    "  for t in xrange(MAX_STEPS):\n",
    "    env.render()\n",
    "    action = pg_reinforce.sampleAction(state[np.newaxis,:])\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_rewards += reward\n",
    "    reward = -10 if done else 0.1 # normalize reward\n",
    "    pg_reinforce.storeRollout(state, action, reward)\n",
    "    #print(\"state\")\n",
    "    #print(state)\n",
    "    #print(\"action\")\n",
    "    #print(action)\n",
    "    #print(\"reward\")\n",
    "    #print(reward)\n",
    "    state = next_state\n",
    "    if done: break\n",
    "\n",
    "  pg_reinforce.updateModel()\n",
    "\n",
    "  episode_history.append(total_rewards)\n",
    "  mean_rewards = np.mean(episode_history)\n",
    "\n",
    "  print(\"Episode {}\".format(i_episode))\n",
    "  print(\"Finished after {} timesteps\".format(t+1))\n",
    "  print(\"Reward for this episode: {}\".format(total_rewards))\n",
    "  print(\"Average reward for last 100 episodes: {}\".format(mean_rewards))\n",
    "  if mean_rewards >= 195.0 and len(episode_history) >= 100:\n",
    "    print(\"Environment {} solved after {} episodes\".format(env_name, i_episode+1))\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
