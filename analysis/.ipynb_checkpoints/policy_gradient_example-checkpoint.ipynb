{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from othello_rules import *\n",
    "from othello_net import *\n",
    "from tensorflow.python.framework import ops\n",
    "from datetime import datetime\n",
    "from example_states import *\n",
    "from feature_extractor import *\n",
    "from training_utils import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy gradient example\n",
      "[[-1. -1. -1.  0.  0. -1. -1. -1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1.  1.  1.  1.  1.]\n",
      " [ 1. -1. -1.  1. -1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1. -1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1. -1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1. -1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"policy gradient example\")\n",
    "test = policy_gradient_example()\n",
    "test = np.transpose(test)\n",
    "print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b10181e2e6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtf_graph_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf_graph_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_example_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0msess_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_graph_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "from rl_reinforce import REINFORCEothello\n",
    "from othello_net import *\n",
    "from othello_rules import *\n",
    "from feature_extractor import *\n",
    "import os, random, sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from example_states import *\n",
    "from training_utils import *\n",
    "from reinforcement_utils import *\n",
    "import random\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "np.set_printoptions(precision=2)\n",
    "tf.reset_default_graph()\n",
    "test = policy_gradient_example()\n",
    "test = np.transpose(test)\n",
    "tf_graph_1 = tf.Graph()\n",
    "with tf_graph_1.as_default():\n",
    "    img_1, train_step_1, adam_1, ground_truth_1, loss_1, pred_1, learn_rate_1, score_1, weights = create_example_graph()\n",
    "sess_1 = tf.Session(graph=tf_graph_1)\n",
    "features = [test]\n",
    "features = np.transpose(features)\n",
    "with sess_1.as_default():\n",
    "    with tf_graph_1.as_default():\n",
    "        rl_reinforce_1 = REINFORCEothello(sess_1, adam_1, learn_rate_1, loss_1, score_1, ground_truth_1, img_1)\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver_1 = tf.train.Saver(tf.global_variables(), max_to_keep=9999)\n",
    "        #model_1 = \"supervised/models/layers8filters64.ckpt\"\n",
    "        #if os.path.isfile(model_1 + \".meta\"):\n",
    "        #    saver_1.restore(sess_1, model_1)\n",
    "        #    print(\"loaded graph 1 as \" + model_1)\n",
    "prediction = sess_1.run(pred_1, feed_dict={img_1:[features]})\n",
    "board = test\n",
    "player = -1\n",
    "x = tf.Print(weights, [weights])\n",
    "v = sess_1.run(weights)\n",
    "print(v)\n",
    "print(player)\n",
    "sampled_move = sample_action(board, player, prediction)\n",
    "label = move_to_label('15')\n",
    "action = label\n",
    "state = features\n",
    "reward = 1\n",
    "action = [action]\n",
    "action = np.transpose(action)\n",
    "print(tf.trainable_variables())\n",
    "for roll in range(0, 2):\n",
    "    rl_reinforce_1.storeRollout(state, action, reward)\n",
    "rl_reinforce_1.updateModel()\n",
    "v = sess_1.run(weights)\n",
    "print(v)\n",
    "prediction = sess_1.run(pred_1, feed_dict={img_1:[features]})\n",
    "board = test\n",
    "player = -1\n",
    "print(board)\n",
    "sampled_move = sample_action(board, player, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "from rl_reinforce import REINFORCEothello\n",
    "from othello_net import *\n",
    "from othello_rules import *\n",
    "from feature_extractor import *\n",
    "import os, random, sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from example_states import *\n",
    "from training_utils import *\n",
    "from reinforcement_utils import *\n",
    "import random\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "np.set_printoptions(precision=2)\n",
    "tf.reset_default_graph()\n",
    "test = policy_gradient_example()\n",
    "test = np.transpose(test)\n",
    "tf_graph_1 = tf.Graph()\n",
    "with tf_graph_1.as_default():\n",
    "    img_1, train_step_1, adam_1, ground_truth_1, loss_1, pred_1, learn_rate_1, score_1 = create_no_graph()\n",
    "sess_1 = tf.Session(graph=tf_graph_1)\n",
    "board = test\n",
    "player = -1\n",
    "features = board_to_input_c(board, player, ['11', '22', '33', '44'])\n",
    "with sess_1.as_default():\n",
    "    with tf_graph_1.as_default():\n",
    "        rl_reinforce_1 = REINFORCEothello(sess_1, adam_1, learn_rate_1, loss_1, score_1, ground_truth_1, img_1)\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver_1 = tf.train.Saver(tf.global_variables(), max_to_keep=9999)\n",
    "        model_1 = \"../supervised/models/layers8filters64.ckpt\"\n",
    "        if os.path.isfile(model_1 + \".meta\"):\n",
    "            saver_1.restore(sess_1, model_1)\n",
    "            print(\"loaded graph 1 as \" + model_1)\n",
    "prediction = sess_1.run(pred_1, feed_dict={img_1:[features]})\n",
    "\n",
    "print(board)\n",
    "print(player)\n",
    "sampled_move = sample_action(board, player, prediction)\n",
    "print(sampled_move)\n",
    "label = move_to_label('15')\n",
    "action = label\n",
    "state = features\n",
    "reward = 1\n",
    "action = [action]\n",
    "action = np.transpose(action)\n",
    "for roll in range(0, 1):\n",
    "    rl_reinforce_1.storeRollout(state, action, reward)\n",
    "rl_reinforce_1.updateModel()\n",
    "\n",
    "prediction = sess_1.run(pred_1, feed_dict={img_1:[features]})\n",
    "board = test\n",
    "player = -1\n",
    "print(board)\n",
    "print(player)\n",
    "sampled_move = sample_action(board, player, prediction)\n",
    "print(sampled_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
