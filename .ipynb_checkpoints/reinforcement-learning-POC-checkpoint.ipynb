{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-08 13:08:16,523] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim: 4\n",
      "num_actions: 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f040a5a1d063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                                        \u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                        \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                                        summary_writer=writer)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mMAX_EPISODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hlynurd/Notebook/Othello/pg_reinforce.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, optimizer, policy_network, current_model, state_dim, num_actions, init_exp, final_exp, anneal_steps, discount_factor, reg_param, max_gradient, summary_writer, summary_every)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_model\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcurrent_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# model components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m       saver_def = builder.build(\n\u001b[1;32m    855\u001b[0m           \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "from pg_reinforce import PolicyGradientREINFORCE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "ops.reset_default_graph()\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "sess = tf.Session()\n",
    "#saver = tf.train.Saver()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\n",
    "writer = tf.train.SummaryWriter(\"/tmp/{}-experiment-1\".format(env_name))\n",
    "\n",
    "state_dim   = env.observation_space.shape[0]\n",
    "print(\"state dim: \" + str(state_dim))\n",
    "num_actions = env.action_space.n\n",
    "print(\"num_actions: \" + str(num_actions))\n",
    "def policy_network(states):\n",
    "  # define policy neural network\n",
    "  W1 = tf.get_variable(\"W1\", [state_dim, 20],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b1 = tf.get_variable(\"b1\", [20],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n",
    "  W2 = tf.get_variable(\"W2\", [20, num_actions],\n",
    "                       initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "  b2 = tf.get_variable(\"b2\", [num_actions],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  p = tf.matmul(h1, W2) + b2\n",
    "  return p\n",
    "current_model = \"cartpole.ckpt\"\n",
    "pg_reinforce = PolicyGradientREINFORCE(sess,\n",
    "                                       optimizer,\n",
    "                                       policy_network,\n",
    "                                       current_model,\n",
    "                                       state_dim,\n",
    "                                       num_actions,\n",
    "                                       summary_writer=writer)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "MAX_STEPS    = 200\n",
    "#sys.exit(\"Error message\")\n",
    "episode_history = deque(maxlen=100)\n",
    "for i_episode in xrange(MAX_EPISODES):\n",
    "\n",
    "  # initialize\n",
    "  state = env.reset()\n",
    "  total_rewards = 0\n",
    "\n",
    "  for t in xrange(MAX_STEPS):\n",
    "    env.render()\n",
    "    action = pg_reinforce.sampleAction(state[np.newaxis,:])\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_rewards += reward\n",
    "    reward = -10 if done else 0.1 # normalize reward\n",
    "    pg_reinforce.storeRollout(state, action, reward)\n",
    "    #print(\"state\")\n",
    "    #print(state)\n",
    "    #print(\"action\")\n",
    "    #print(action)\n",
    "    #print(\"reward\")\n",
    "    #print(reward)\n",
    "    state = next_state\n",
    "    if done: break\n",
    "\n",
    "  pg_reinforce.updateModel()\n",
    "\n",
    "  episode_history.append(total_rewards)\n",
    "  mean_rewards = np.mean(episode_history)\n",
    "\n",
    "  print(\"Episode {}\".format(i_episode))\n",
    "  print(\"Finished after {} timesteps\".format(t+1))\n",
    "  print(\"Reward for this episode: {}\".format(total_rewards))\n",
    "  print(\"Average reward for last 100 episodes: {}\".format(mean_rewards))\n",
    "  if mean_rewards >= 195.0 and len(episode_history) >= 100:\n",
    "    print(\"Environment {} solved after {} episodes\".format(env_name, i_episode+1))\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
