{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "from rl_reinforce import REINFORCEothello\n",
    "from othello_net import *\n",
    "from othello_rules import *\n",
    "from feature_extractor import *\n",
    "import os, random, sys, gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from example_states import *\n",
    "from training_utils import *\n",
    "np.set_printoptions(precision=2)\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_action(prediction):\n",
    "    prediction = np.transpose(prediction[0])\n",
    "    prediction = np.transpose(prediction[1])\n",
    "    legal_moves = find_legal_moves(board, player)\n",
    "    prediciton = prediction / np.sum(prediction)\n",
    "    cleaned_predictions = zero_illegal_moves(prediction, legal_moves)\n",
    "    p = cleaned_predictions.flatten()\n",
    "    p = p / np.sum(p)\n",
    "    sample_index = np.flatnonzero( np.random.multinomial(1,p,1) )[0]\n",
    "    sampled_move = moves[sample_index]\n",
    "    if prev_move == sampled_move:\n",
    "        print(\"action probabilities\")\n",
    "        print(p)\n",
    "        print(prediction)\n",
    "        print(cleaned_predictions)\n",
    "    return sampled_move\n",
    "\n",
    "moves = ['0'] * 64\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        moves[i*8 + j] = str((i+1) * 10 + (j+1))\n",
    "        \n",
    "\n",
    "batches = 1000\n",
    "games_per_batch = 10\n",
    "test_game_batch = 20\n",
    "initial_a_model = \"models/rl-p-a/a_player_0.ckpt\"\n",
    "initial_b_model = \"models/rl-p-b/b_player_0.ckpt\"\n",
    "\n",
    "cnt = 90\n",
    "action_buffer = []\n",
    "state_buffer = []\n",
    "reward_buffer = []\n",
    "print(\"starting rl policy network\")\n",
    "for batch in range(batches):\n",
    "    # TODO: Choose the initial b model as the opponent once in a while to see how\n",
    "    # well our learner does agains it now\n",
    "    ops.reset_default_graph()\n",
    "    # Define the variables for our main learner\n",
    "    graph_1, img_data_1, train_step_1, optimizer_1, ground_truths_1, loss_1, pred_up_1, keep_prob_1, learn_rate_1, score_out_1 = create_othello_net()\n",
    "    sess_1 = tf.Session(graph=graph_1)\n",
    "    saver_1 = tf.train.Saver()\n",
    "    init_op_1 = tf.initialize_all_variables()\n",
    "    sess_1.run(init_op_1)\n",
    "    model_1 = \"models/rl-p-a/a_player_\" + str(cnt-1) + \".ckpt\"\n",
    "    if os.path.isfile(model_1):\n",
    "        saver_1.restore(sess_1, model_1)\n",
    "    rl_reinforce_1 = REINFORCEothello(sess_1,\n",
    "                                           optimizer_1,\n",
    "                                           learn_rate_1,\n",
    "                                           keep_prob_1,\n",
    "                                           loss_1,\n",
    "                                           score_out_1,\n",
    "                                           ground_truths_1,\n",
    "                                           img_data_1)\n",
    "    \n",
    "    # Define the variables for the randomly chosen opponent\n",
    "    graph_2, img_data_2, train_step_2, optimizer_2, ground_truths_2, loss_2, pred_up_2, keep_prob_2, learn_rate_2, score_out_2 = create_othello_net()\n",
    "    sess_2 = tf.Session(graph=graph_2)\n",
    "    saver_2 = tf.train.Saver()\n",
    "    init_op_2 = tf.initialize_all_variables()\n",
    "    sess_2.run(init_op_2)\n",
    "    if batch % test_game_batch == 0:\n",
    "        model_2 = 'models/tiny-rl-j.ckpt'\n",
    "        games_per_batch = 100\n",
    "    else:\n",
    "        all_files = os.listdir(\"models/rl-p-b/\")\n",
    "        no_meta = [i for i in all_files if not ('meta' in i or 'checkpoint' in i)]\n",
    "        model_2 = 'models/rl-p-b/' + random.choice(no_meta)\n",
    "        games_per_batch = 10\n",
    "    if os.path.isfile(model_2):\n",
    "        saver_2.restore(sess_2, model_2)\n",
    "    #rl_reinforce_2 = REINFORCEothello(sess_2,\n",
    "    #                                   optimizer_2,\n",
    "    #                                   learn_rate_2,\n",
    "    #                                   keep_prob_2,\n",
    "    #                                   loss_2,\n",
    "    #                                   score_out_2,\n",
    "    #                                   ground_truths_2,\n",
    "    #                                   img_data_2)\n",
    "\n",
    "    N = games_per_batch\n",
    "    graph_1 = -1\n",
    "    graph_2 = 1\n",
    "    graph_1_wins = 0\n",
    "    avg_vector_1 = []\n",
    "    graph_2_wins = 0\n",
    "    avg_vector_2 = []\n",
    "    temp_reset = True\n",
    "    for n in range(N):\n",
    "        #TODO: Every once in a while play games against an independent\n",
    "        # othello player and see how the standing is - this is the true\n",
    "        # measure of the success of the RL part\n",
    "        #TODO: Make a schedule as a function of current batch\n",
    "        # then I have to let the REINFORCEothello take the learning rate\n",
    "        # as a parameter\n",
    "        board = initialize_game()\n",
    "        player = -1\n",
    "        prev_move = '00'\n",
    "        input_batch = []\n",
    "        label_batch = []\n",
    "        while True:\n",
    "            legal_moves = find_legal_moves(board, player)\n",
    "            if len(legal_moves) == 0:\n",
    "                winner = get_winner(board, 1, -1)\n",
    "                if winner is graph_1:\n",
    "                    graph_1_wins += 1\n",
    "                if winner is graph_2:\n",
    "                    graph_2_wins += 1\n",
    "                break\n",
    "            features = board_to_input(board, player)\n",
    "            if player is graph_1:\n",
    "                prediction = sess_1.run(pred_up_1, feed_dict={img_data_1:[features], keep_prob_1:1.0})\n",
    "            else:\n",
    "                prediction = sess_2.run(pred_up_2, feed_dict={img_data_2:[features], keep_prob_2:1.0})\n",
    "\n",
    "\n",
    "            sampled_move = sample_action(prediction)\n",
    "            if prev_move == sampled_move:\n",
    "                print(\"illegal action sampled:\")\n",
    "                print(sampled_move)\n",
    "                print(\"in state\")\n",
    "                print(board)\n",
    "                break\n",
    "            prev_move = sampled_move\n",
    "            label = prepare_data(move_to_label(sampled_move))\n",
    "            input_batch.append(features)\n",
    "            label_batch.append(label)\n",
    "\n",
    "            board = make_move(board, sampled_move, player, debug=True)\n",
    "\n",
    "            if player is 1:\n",
    "                player = -1\n",
    "            else:\n",
    "                player = 1\n",
    "            legal_moves = find_legal_moves(board, player)\n",
    "            if len(legal_moves) == 0:\n",
    "                if player is 1:\n",
    "                    player = -1\n",
    "                else:\n",
    "                    player = 1\n",
    "        if winner:\n",
    "            for j in range(len(input_batch)):\n",
    "                # TODO: Write a prepare_training_batch function that also \n",
    "                # does flips, use that here and in the supervised-learning script\n",
    "                # but make sure that the rewards match the amount of augmentation!\n",
    "                # append the same reward four times before flipping, or something\n",
    "                # like this. Also remember to divide the learning rate then by 4.\n",
    "                state = input_batch[j]\n",
    "                action = label_batch[j]\n",
    "                if winner is -1:\n",
    "                    reward = 1 if j% 2 is 0 else -0.1\n",
    "                else:\n",
    "                    reward = -0.1 if j% 2 is 0 else 1\n",
    "                if batch % test_game_batch != 0:\n",
    "                    rl_reinforce_1.storeRollout(state, action, reward)\n",
    "                    action_buffer.append(action)\n",
    "                    state_buffer.append(state)\n",
    "                    reward_buffer.append(reward)\n",
    "                    #rl_reinforce_2.storeRollout(state, action, reward)\n",
    "        graph_1 = graph_1 * (-1)\n",
    "        graph_2 = graph_2 * (-1)\n",
    "        if n+1 is games_per_batch:\n",
    "            print('%s  %s wr: %.2f, %s wr: %.2f' % (datetime.now().strftime(\"%d. %H:%M:%S\"), model_1[14:], graph_1_wins/float(N), model_2[14:], graph_2_wins/float(N)))\n",
    "            graph_1_wins = 0\n",
    "            graph_2_wins = 0\n",
    "        if n+1 is games_per_batch and batch % test_game_batch != 0:\n",
    "            rl_reinforce_1.updateModel()\n",
    "            #rl_reinforce_2.updateModel()\n",
    "            save_path = saver_1.save(sess_1, \"models/rl-p-a/a_player_\" + str(cnt) + \".ckpt\") \n",
    "            #save_path = saver_2.save(sess_2, \"models/rl-p-b/b_player_\" + str(batch) + \".ckpt\")             \n",
    "\n",
    "            \n",
    "            ops.reset_default_graph()\n",
    "            # Define these variables for the tensorflow name scope\n",
    "            graph_1, img_data_1, train_step_1, optimizer_1, ground_truths_1, loss_1, pred_up_1, keep_prob_1, learn_rate_1, score_out_1 = create_othello_net()\n",
    "            sess_1 = tf.Session(graph=graph_1)\n",
    "            saver_1 = tf.train.Saver()\n",
    "            init_op_1 = tf.initialize_all_variables()\n",
    "            sess_1.run(init_op_1)\n",
    "            # Now define the graph for the newest addition into our opponent pool\n",
    "            # we let him watch all the games that were played between the randomly chosen\n",
    "            # opponent and our main learner, update and save as a new model\n",
    "            graph_2, img_data_2, train_step_2, optimizer_2, ground_truths_2, loss_2, pred_up_2, keep_prob_2, learn_rate_2, score_out_2 = create_othello_net()\n",
    "            sess_2 = tf.Session(graph=graph_2)\n",
    "            saver_2 = tf.train.Saver()\n",
    "            init_op_2 = tf.initialize_all_variables()\n",
    "            sess_2.run(init_op_2)\n",
    "            # XXX: \"not finding the model for some reason\" happens after every test \n",
    "            # should rather have a cnt rather than let it go by batch\n",
    "            model_2 = \"models/rl-p-b/b_player_\" + str(cnt-1) + \".ckpt\"\n",
    "            if os.path.isfile(model_2):\n",
    "                saver_2.restore(sess_2, model_2)\n",
    "            else:\n",
    "                print(\"not finding the model for some reason\")\n",
    "            rl_reinforce_2 = REINFORCEothello(sess_2,\n",
    "                                               optimizer_2,\n",
    "                                               learn_rate_2,\n",
    "                                               keep_prob_2,\n",
    "                                               loss_2,\n",
    "                                               score_out_2,\n",
    "                                               ground_truths_2,\n",
    "                                               img_data_2)\n",
    "            for buff in range(len(action_buffer)):\n",
    "                rl_reinforce_2.storeRollout(state_buffer[buff], action_buffer[buff], reward_buffer[buff])\n",
    "            rl_reinforce_2.updateModel()\n",
    "            save_path = saver_2.save(sess_2, \"models/rl-p-b/b_player_\" + str(cnt) + \".ckpt\")    \n",
    "            cnt += 1\n",
    "            action_buffer = []\n",
    "            state_buffer = []\n",
    "            reward_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a_player_0 seems to win 40-49 % of games against j\n",
    "\n",
    "#trained 1-67    with 5e-5\n",
    "#trained 68-81   with 2e-5\n",
    "#12. 11:52:10  a_player_66.ckpt wr: 0.44, -j.ckpt wr: 0.50\n",
    "#12. 11:56:59  a_player_70.ckpt wr: 0.42, -j.ckpt wr: 0.54\n",
    "#12. 12:02:07  a_player_74.ckpt wr: 0.38, -j.ckpt wr: 0.54\n",
    "#12. 12:06:43  a_player_78.ckpt wr: 0.36, -j.ckpt wr: 0.62\n",
    "#trained 81-     with 3e-5 but 1/5 of the games per opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
